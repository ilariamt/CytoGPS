{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e978be3",
   "metadata": {},
   "source": [
    "# CytoGPS Survival Analysis Demo\n",
    "## Random Survival Forest and Transformer Survival on AnnData\n",
    "\n",
    "This notebook demonstrates survival analysis on AnnData object with:\n",
    "- **Observations:** 1,222 samples\n",
    "- **Variables:** 2,748 cytogenetic features\n",
    "- **Task:** Compare Random Survival Forest and Transformer-based Survival models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f8ef4",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore AnnData Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ac3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import concordance_index_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f13558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic AnnData object matching your specifications\n",
    "n_obs = 1222\n",
    "n_vars = 2748\n",
    "\n",
    "# Generate random feature matrix\n",
    "X = np.random.randn(n_obs, n_vars)\n",
    "\n",
    "# Create observation metadata\n",
    "obs_data = {\n",
    "    'Line_Number': np.arange(n_obs),\n",
    "    'pat_id': np.random.randint(1, 300, n_obs),\n",
    "    'Clone_Number': np.random.randint(1, 10, n_obs),\n",
    "    'Karyotype_Revised': [f'kar_{i}' for i in range(n_obs)],\n",
    "    'Clone_Code': np.random.randint(1, 100, n_obs),\n",
    "    'survival_time': np.random.exponential(scale=50, size=n_obs) + 10,  # months\n",
    "    'event_occurred': np.random.binomial(1, 0.6, n_obs),  # 60% event rate\n",
    "    'sex': np.random.choice(['M', 'F'], n_obs),\n",
    "    'age': np.random.uniform(30, 85, n_obs),\n",
    "    'n_abs_aberr': np.random.randint(0, 15, n_obs),\n",
    "    'n_abs_aberr_iscn': np.random.randint(0, 15, n_obs),\n",
    "    'n_abs_max_clone': np.random.randint(0, 15, n_obs),\n",
    "    'ck_abs': np.random.randint(0, 10, n_obs),\n",
    "    'ck_abs_iscn': np.random.randint(0, 10, n_obs),\n",
    "    'ck_abs_max_clone': np.random.randint(0, 10, n_obs),\n",
    "    'diff_max_clone': np.random.randint(0, 5, n_obs),\n",
    "    'ck_category': np.random.choice(['good', 'intermediate', 'poor'], n_obs),\n",
    "    'ck_category_iscn': np.random.choice(['good', 'intermediate', 'poor'], n_obs),\n",
    "    'ck_category_max_clone': np.random.choice(['good', 'intermediate', 'poor'], n_obs),\n",
    "    'clonal_evolution': np.random.binomial(1, 0.5, n_obs),\n",
    "    'TP53mutStatus': np.random.choice(['WT', 'MUT', 'NA'], n_obs),\n",
    "    'IGHVmutStatus': np.random.choice(['M', 'UM', 'NA'], n_obs),\n",
    "    'percent_path_cells_ip': np.random.uniform(0, 100, n_obs),\n",
    "    'material': np.random.choice(['PB', 'BM', 'LN'], n_obs),\n",
    "    'bal_transloc': np.random.randint(0, 5, n_obs),\n",
    "    'unbal_transloc': np.random.randint(0, 5, n_obs),\n",
    "    'deletions': np.random.randint(0, 5, n_obs),\n",
    "    'der_chrom': np.random.randint(0, 5, n_obs),\n",
    "    'only_bal_abs_trisomies': np.random.randint(0, 5, n_obs),\n",
    "    'comma_abs_count': np.random.randint(0, 100, n_obs),\n",
    "    'character_count': np.random.randint(100, 500, n_obs),\n",
    "    'any_uncertain_abs': np.random.binomial(1, 0.3, n_obs),\n",
    "    'any_detailed_abs': np.random.binomial(1, 0.5, n_obs),\n",
    "    'LGF_subcytoband_sum': np.random.uniform(0, 50, n_obs),\n",
    "    'Loss_cytoband_sum': np.random.uniform(0, 50, n_obs),\n",
    "    'Gain_cytoband_sum': np.random.uniform(0, 50, n_obs),\n",
    "    'Fusion_cytoband_sum': np.random.uniform(0, 50, n_obs),\n",
    "    'LGF_cytoband_sum': np.random.uniform(0, 50, n_obs),\n",
    "}\n",
    "\n",
    "obs_df = pd.DataFrame(obs_data)\n",
    "\n",
    "# Create variable names\n",
    "var_names = [f'feature_{i}' for i in range(n_vars)]\n",
    "\n",
    "# Create obsm data (observation matrices for different cytoband categories)\n",
    "obsm_data = {\n",
    "    'Fusion_cytoband': np.random.randn(n_obs, 100),\n",
    "    'Fusion_subcytoband': np.random.randn(n_obs, 80),\n",
    "    'Gain_cytoband': np.random.randn(n_obs, 120),\n",
    "    'Gain_subcytoband': np.random.randn(n_obs, 100),\n",
    "    'Loss_cytoband': np.random.randn(n_obs, 110),\n",
    "    'Loss_subcytoband': np.random.randn(n_obs, 90),\n",
    "}\n",
    "\n",
    "# Create uns data (unstructured annotation)\n",
    "uns_data = {\n",
    "    'Fusion_cytoband_varnames': [f'Fusion_cytoband_{i}' for i in range(100)],\n",
    "    'Fusion_subcytoband_varnames': [f'Fusion_subcytoband_{i}' for i in range(80)],\n",
    "    'Gain_cytoband_varnames': [f'Gain_cytoband_{i}' for i in range(120)],\n",
    "    'Gain_subcytoband_varnames': [f'Gain_subcytoband_{i}' for i in range(100)],\n",
    "    'Loss_cytoband_varnames': [f'Loss_cytoband_{i}' for i in range(110)],\n",
    "    'Loss_subcytoband_varnames': [f'Loss_subcytoband_{i}' for i in range(90)],\n",
    "}\n",
    "\n",
    "# Create AnnData object\n",
    "adata = ad.AnnData(X=X, obs=obs_df, var=pd.DataFrame(index=var_names), obsm=obsm_data, uns=uns_data)\n",
    "\n",
    "print(\"AnnData Object Created Successfully!\")\n",
    "print(f\"\\nAnnData Structure:\")\n",
    "print(adata)\n",
    "print(f\"\\nShape: {adata.n_obs} observations × {adata.n_vars} variables\")\n",
    "print(f\"\\nObservation Metadata (obs):\")\n",
    "print(adata.obs.head())\n",
    "print(f\"\\nSurvival Data Summary:\")\n",
    "print(adata.obs[['survival_time', 'event_occurred']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9151e1",
   "metadata": {},
   "source": [
    "## Section 2: Prepare Data for Survival Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from adata.X\n",
    "X_features = adata.X.astype(np.float32)\n",
    "\n",
    "# Extract survival outcomes\n",
    "survival_time = adata.obs['survival_time'].values\n",
    "event_occurred = adata.obs['event_occurred'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X_features.shape}\")\n",
    "print(f\"Survival time shape: {survival_time.shape}\")\n",
    "print(f\"Event occurred shape: {event_occurred.shape}\")\n",
    "print(f\"\\nSurvival Statistics:\")\n",
    "print(f\"  Mean survival time: {survival_time.mean():.2f} months\")\n",
    "print(f\"  Median survival time: {np.median(survival_time):.2f} months\")\n",
    "print(f\"  Event rate: {event_occurred.mean():.2%}\")\n",
    "\n",
    "# Handle missing values (none in this synthetic case, but good practice)\n",
    "print(f\"\\nMissing values in features: {np.isnan(X_features).sum()}\")\n",
    "print(f\"Missing values in survival_time: {np.isnan(survival_time).sum()}\")\n",
    "print(f\"Missing values in event_occurred: {np.isnan(event_occurred).sum()}\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_features_normalized = scaler.fit_transform(X_features)\n",
    "\n",
    "print(f\"\\nFeatures normalized:\")\n",
    "print(f\"  Mean: {X_features_normalized.mean():.6f}\")\n",
    "print(f\"  Std: {X_features_normalized.std():.6f}\")\n",
    "\n",
    "# Split data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, time_train, time_test, event_train, event_test = train_test_split(\n",
    "    X_features_normalized, survival_time, event_occurred,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318d84b",
   "metadata": {},
   "source": [
    "## Section 3: Train Random Survival Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ece08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scikit-survival if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from sksurv.ensemble import RandomSurvivalForest\n",
    "    from sksurv.metrics import concordance_index_censored\n",
    "except ImportError:\n",
    "    print(\"Installing scikit-survival...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-survival\", \"-q\"])\n",
    "    from sksurv.ensemble import RandomSurvivalForest\n",
    "    from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# Prepare structured array for survival data (required by sksurv)\n",
    "structured_time_train = np.array([(bool(e), t) for e, t in zip(event_train, time_train)],\n",
    "                                  dtype=[('event', bool), ('time', float)])\n",
    "structured_time_test = np.array([(bool(e), t) for e, t in zip(event_test, time_test)],\n",
    "                                 dtype=[('event', bool), ('time', float)])\n",
    "\n",
    "print(\"Training Random Survival Forest Model...\")\n",
    "\n",
    "# Initialize and train Random Survival Forest\n",
    "rsf = RandomSurvivalForest(\n",
    "    n_estimators=100,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rsf.fit(X_train, structured_time_train)\n",
    "\n",
    "print(\"Random Survival Forest training completed!\")\n",
    "print(f\"Model parameters:\")\n",
    "print(f\"  Number of trees: {rsf.n_estimators}\")\n",
    "print(f\"  Max depth: {rsf.max_depth}\")\n",
    "print(f\"  Min samples split: {rsf.min_samples_split}\")\n",
    "\n",
    "# Generate predictions on test set\n",
    "rsf_predictions = rsf.predict(X_test)\n",
    "\n",
    "# Calculate concordance index for RSF\n",
    "rsf_cindex = concordance_index_censored(event_test, time_test, rsf_predictions)[0]\n",
    "\n",
    "print(f\"\\nRandom Survival Forest Performance:\")\n",
    "print(f\"  Concordance Index: {rsf_cindex:.4f}\")\n",
    "print(f\"  Predictions shape: {rsf_predictions.shape}\")\n",
    "print(f\"  Prediction range: [{rsf_predictions.min():.4f}, {rsf_predictions.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88bad3",
   "metadata": {},
   "source": [
    "## Section 4: Train Transformer Survival Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97add2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torch and pycox for Transformer Survival models\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "except ImportError:\n",
    "    print(\"Installing PyTorch...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"-q\"])\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "try:\n",
    "    from pycox.models import DeepHitSingle\n",
    "    from pycox.preprocessing.label_transforms import LabelTransformDiscrete\n",
    "except ImportError:\n",
    "    print(\"Installing pycox...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pycox\", \"-q\"])\n",
    "    from pycox.models import DeepHitSingle\n",
    "    from pycox.preprocessing.label_transforms import LabelTransformDiscrete\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train).to(device)\n",
    "X_test_torch = torch.FloatTensor(X_test).to(device)\n",
    "time_train_torch = torch.FloatTensor(time_train).to(device)\n",
    "time_test_torch = torch.FloatTensor(time_test).to(device)\n",
    "event_train_torch = torch.FloatTensor(event_train).to(device)\n",
    "event_test_torch = torch.FloatTensor(event_test).to(device)\n",
    "\n",
    "# Define simple Transformer-based Survival Model\n",
    "class TransformerSurvivalModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_heads=8, num_layers=2, dropout=0.1):\n",
    "        super(TransformerSurvivalModel, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Project input to hidden dimension\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Add sequence dimension for transformer (batch, seq, features)\n",
    "        x = x.unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "        \n",
    "        # Apply transformer\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Pool: take the mean of the sequence\n",
    "        x = x.mean(dim=1)  # (batch, hidden_dim)\n",
    "        \n",
    "        # Output layer - positive output for survival risk\n",
    "        x = self.fc_layers(x)\n",
    "        x = torch.relu(x)  # Ensure positive risk scores\n",
    "        \n",
    "        return x.squeeze(1)\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing Transformer Survival Model...\")\n",
    "transformer_model = TransformerSurvivalModel(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dim=256,\n",
    "    num_heads=8,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Loss function: Cox partial likelihood\n",
    "class CoxLoss(nn.Module):\n",
    "    def forward(self, hazard, time, event):\n",
    "        # Sort by time\n",
    "        sorted_indices = torch.argsort(time, descending=True)\n",
    "        hazard = hazard[sorted_indices]\n",
    "        time = time[sorted_indices]\n",
    "        event = event[sorted_indices]\n",
    "        \n",
    "        # Calculate Cox partial likelihood\n",
    "        log_hazard = torch.log(hazard + 1e-8)\n",
    "        cumsum_hazard = torch.cumsum(torch.exp(hazard), dim=0)\n",
    "        \n",
    "        loss = -torch.sum(event * (log_hazard - torch.log(cumsum_hazard + 1e-8)))\n",
    "        return loss / (torch.sum(event) + 1e-8)\n",
    "\n",
    "criterion = CoxLoss()\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training Transformer Survival Model...\")\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_torch, time_train_torch, event_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    transformer_model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_X, batch_time, batch_event in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        hazard = transformer_model(batch_X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(hazard, batch_time, batch_event)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "print(\"Transformer Survival Model training completed!\")\n",
    "\n",
    "# Generate predictions on test set\n",
    "transformer_model.eval()\n",
    "with torch.no_grad():\n",
    "    transformer_predictions = transformer_model(X_test_torch).cpu().numpy()\n",
    "\n",
    "# Calculate concordance index for Transformer\n",
    "transformer_cindex = concordance_index_censored(event_test, time_test, transformer_predictions)[0]\n",
    "\n",
    "print(f\"\\nTransformer Survival Model Performance:\")\n",
    "print(f\"  Concordance Index: {transformer_cindex:.4f}\")\n",
    "print(f\"  Predictions shape: {transformer_predictions.shape}\")\n",
    "print(f\"  Prediction range: [{transformer_predictions.min():.4f}, {transformer_predictions.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0c4e3",
   "metadata": {},
   "source": [
    "## Section 5: Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import integrated_brier_score, brier_score\n",
    "from scipy.special import expit\n",
    "\n",
    "# Calculate additional survival metrics\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Concordance Index (higher is better, max=1.0)\n",
    "print(\"\\n1. CONCORDANCE INDEX (Harrell's C-index)\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   Random Survival Forest:     {rsf_cindex:.4f}\")\n",
    "print(f\"   Transformer Survival:       {transformer_cindex:.4f}\")\n",
    "print(f\"   Baseline (random):          0.5000\")\n",
    "\n",
    "# Determine which model is better\n",
    "if rsf_cindex > transformer_cindex:\n",
    "    print(f\"   ✓ RSF is better by: {(rsf_cindex - transformer_cindex):.4f}\")\n",
    "else:\n",
    "    print(f\"   ✓ Transformer is better by: {(transformer_cindex - rsf_cindex):.4f}\")\n",
    "\n",
    "# 2. Risk Score Statistics\n",
    "print(\"\\n2. RISK SCORE STATISTICS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "rsf_risk_scores = rsf_predictions\n",
    "transformer_risk_scores = transformer_predictions\n",
    "\n",
    "print(f\"   Random Survival Forest:\")\n",
    "print(f\"      Mean:              {rsf_risk_scores.mean():.4f}\")\n",
    "print(f\"      Std Dev:           {rsf_risk_scores.std():.4f}\")\n",
    "print(f\"      Min:               {rsf_risk_scores.min():.4f}\")\n",
    "print(f\"      Max:               {rsf_risk_scores.max():.4f}\")\n",
    "print(f\"      Median:            {np.median(rsf_risk_scores):.4f}\")\n",
    "\n",
    "print(f\"\\n   Transformer Survival:\")\n",
    "print(f\"      Mean:              {transformer_risk_scores.mean():.4f}\")\n",
    "print(f\"      Std Dev:           {transformer_risk_scores.std():.4f}\")\n",
    "print(f\"      Min:               {transformer_risk_scores.min():.4f}\")\n",
    "print(f\"      Max:               {transformer_risk_scores.max():.4f}\")\n",
    "print(f\"      Median:            {np.median(transformer_risk_scores):.4f}\")\n",
    "\n",
    "# 3. Create performance comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Concordance Index', 'Mean Risk Score', 'Std Dev Risk Score', 'Min Risk Score', 'Max Risk Score'],\n",
    "    'RSF': [\n",
    "        rsf_cindex,\n",
    "        rsf_risk_scores.mean(),\n",
    "        rsf_risk_scores.std(),\n",
    "        rsf_risk_scores.min(),\n",
    "        rsf_risk_scores.max()\n",
    "    ],\n",
    "    'Transformer': [\n",
    "        transformer_cindex,\n",
    "        transformer_risk_scores.mean(),\n",
    "        transformer_risk_scores.std(),\n",
    "        transformer_risk_scores.min(),\n",
    "        transformer_risk_scores.max()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n3. PERFORMANCE COMPARISON TABLE\")\n",
    "print(\"-\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 4. Stratify patients by risk quartiles\n",
    "print(\"\\n4. PATIENT STRATIFICATION BY RISK QUARTILES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name, risk_scores in [(\"RSF\", rsf_risk_scores), (\"Transformer\", transformer_risk_scores)]:\n",
    "    quartiles = np.percentile(risk_scores, [25, 50, 75])\n",
    "    q1 = (risk_scores <= quartiles[0]).sum()\n",
    "    q2 = ((risk_scores > quartiles[0]) & (risk_scores <= quartiles[1])).sum()\n",
    "    q3 = ((risk_scores > quartiles[1]) & (risk_scores <= quartiles[2])).sum()\n",
    "    q4 = (risk_scores > quartiles[2]).sum()\n",
    "    \n",
    "    print(f\"\\n   {model_name}:\")\n",
    "    print(f\"      Q1 (Low risk):     {q1:4d} patients ({q1/len(risk_scores):.1%})\")\n",
    "    print(f\"      Q2 (Mid-Low risk): {q2:4d} patients ({q2/len(risk_scores):.1%})\")\n",
    "    print(f\"      Q3 (Mid-High risk):{q3:4d} patients ({q3/len(risk_scores):.1%})\")\n",
    "    print(f\"      Q4 (High risk):    {q4:4d} patients ({q4/len(risk_scores):.1%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaadf20",
   "metadata": {},
   "source": [
    "## Section 6: Visualize Survival Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Survival Analysis: Random Survival Forest vs Transformer Model', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Risk Score Distribution Comparison\n",
    "ax = axes[0, 0]\n",
    "ax.hist(rsf_risk_scores, bins=30, alpha=0.6, label='RSF', color='steelblue', edgecolor='black')\n",
    "ax.hist(transformer_risk_scores, bins=30, alpha=0.6, label='Transformer', color='coral', edgecolor='black')\n",
    "ax.set_xlabel('Risk Score', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Number of Patients', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Distribution of Risk Scores', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 2. Risk Score Correlation\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(rsf_risk_scores, transformer_risk_scores, alpha=0.5, s=30, color='purple')\n",
    "correlation = np.corrcoef(rsf_risk_scores, transformer_risk_scores)[0, 1]\n",
    "ax.set_xlabel('RSF Risk Score', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Transformer Risk Score', fontsize=11, fontweight='bold')\n",
    "ax.set_title(f'Model Agreement (Correlation: {correlation:.3f})', fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Add diagonal line\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.5, zorder=0)\n",
    "\n",
    "# 3. Kaplan-Meier curves for RSF risk quartiles\n",
    "ax = axes[1, 0]\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "rsf_quartiles = np.percentile(rsf_risk_scores, [25, 50, 75])\n",
    "rsf_risk_groups = np.digitize(rsf_risk_scores, rsf_quartiles)\n",
    "\n",
    "colors = ['green', 'yellow', 'orange', 'red']\n",
    "group_labels = ['Q1 (Low)', 'Q2 (Mid-Low)', 'Q3 (Mid-High)', 'Q4 (High)']\n",
    "\n",
    "for group_idx in range(1, 5):\n",
    "    mask = rsf_risk_groups == group_idx\n",
    "    kmf.fit(time_test[mask], event_test[mask], label=group_labels[group_idx-1])\n",
    "    kmf.plot_survival_function(ax=ax, ci_show=False, linewidth=2.5, color=colors[group_idx-1])\n",
    "\n",
    "ax.set_xlabel('Time (months)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Survival Probability', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Kaplan-Meier: RSF Risk Stratification', fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(loc='lower left', fontsize=10)\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "# 4. Kaplan-Meier curves for Transformer risk quartiles\n",
    "ax = axes[1, 1]\n",
    "kmf_t = KaplanMeierFitter()\n",
    "\n",
    "transformer_quartiles = np.percentile(transformer_risk_scores, [25, 50, 75])\n",
    "transformer_risk_groups = np.digitize(transformer_risk_scores, transformer_quartiles)\n",
    "\n",
    "for group_idx in range(1, 5):\n",
    "    mask = transformer_risk_groups == group_idx\n",
    "    kmf_t.fit(time_test[mask], event_test[mask], label=group_labels[group_idx-1])\n",
    "    kmf_t.plot_survival_function(ax=ax, ci_show=False, linewidth=2.5, color=colors[group_idx-1])\n",
    "\n",
    "ax.set_xlabel('Time (months)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Survival Probability', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Kaplan-Meier: Transformer Risk Stratification', fontsize=12, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend(loc='lower left', fontsize=10)\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('survival_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Survival comparison plots saved as 'survival_comparison.png'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
